# CA_PySpark
Code Academy Project with PySpark
Analyze Common Crawl Data with PySpark
The Common Crawl is a non-profit organization that crawls, archives, and analyzes content on all public websites. The Common Crawl maintains petabytes (thousands of terabytes!) of web content and insights derived from their analyses, all of which is made publicly available for research and educational purposes.

In this project, we’re going to be working with a small portion of a dataset published by the Common Crawl. Analyzing Common Crawl data can be easier if you know a few key facts about the format of domain names. Every website’s name is composed of multiple parts, as illustrated in the following diagram:

Diagram showing the website URL "https://www.codecademy.com". The protocol is "https". The sub-domain is "www". The second-level domain is "codecademy". The top-level domain is "com".

The dataset we’ll be working with, the domain graph, contains a record of every domain on the internet and the count of subdomains associated with the site.

We encourage you to try this on your own, but we’ve included a solution file named solution.ipynb as well that you can access by clicking on the Jupyter logo at the top left of the web browser within the learning environment. Or select the View Solution Notebook link at the top of the notebook to view the solution as an HTML file in a new window.

Be sure to save your work as you go by clicking on the save icon. You can also download a copy of your notebook to reference later by navigating to File>Download as>Notebook (.ipynb). If you would like to try this project off-platform, check out our PySpark installation article.

This project was done in a Jupyter Notebook and the steps will be listed there. 