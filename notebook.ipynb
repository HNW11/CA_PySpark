{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31362fa",
   "metadata": {},
   "source": [
    "# PySpark and Big Data Project \n",
    "\n",
    "* [Project Page Link](https://www.codecademy.com/courses/big-data-pyspark/projects/pyspark-common-crawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f30bf",
   "metadata": {},
   "source": [
    "## Task Group 1 - Analyzing Common Crawl Data with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c96e6",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Initialize a new Spark Context and read in the domain graph as an RDD.\n",
    "\n",
    "\n",
    "One of your colleagues has made good progress analyzing this dataset using only PySpark RDDs, but has asked you to continue work on this project with SparkSQL. To get familiar with the dataset, you should run their analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8d3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ba7d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['367855\\t172-in-addr\\tarpa\\t1',\n",
       " '367856\\taddr\\tarpa\\t1',\n",
       " '367857\\tamphic\\tarpa\\t1',\n",
       " '367858\\tbeta\\tarpa\\t1',\n",
       " '367859\\tcallic\\tarpa\\t1',\n",
       " '367860\\tch\\tarpa\\t1',\n",
       " '367861\\td\\tarpa\\t1',\n",
       " '367862\\thome\\tarpa\\t7',\n",
       " '367863\\tiana\\tarpa\\t1',\n",
       " '367907\\tlocal\\tarpa\\t1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Domains CSV File into an RDD\n",
    "common_crawl_domain_counts = sc.textFile('./crawl/cc-main-limited-domains.csv')\n",
    "\n",
    "# Display first few domains from the RDD\n",
    "common_crawl_domain_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18a601",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Your colleague has written a function called fmt_domain_graph_entry that formats an entry in the domain dataset.\n",
    "\n",
    "Apply `fmt_domain_graph_entry` over `common_crawl_domain_counts` and save the result as a new RDD named `formatted_host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7950d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_domain_graph_entry(entry):\n",
    "    \"\"\"\n",
    "    Formats a Common Crawl domain graph entry. Extracts the site_id, \n",
    "    top-level domain (tld), domain name, and subdomain count as seperate items.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')        \n",
    "    return int(site_id), domain, tld, int(num_subdomains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f9ed9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(367855, '172-in-addr', 'arpa', 1),\n",
       " (367856, 'addr', 'arpa', 1),\n",
       " (367857, 'amphic', 'arpa', 1),\n",
       " (367858, 'beta', 'arpa', 1),\n",
       " (367859, 'callic', 'arpa', 1),\n",
       " (367860, 'ch', 'arpa', 1),\n",
       " (367861, 'd', 'arpa', 1),\n",
       " (367862, 'home', 'arpa', 7),\n",
       " (367863, 'iana', 'arpa', 1),\n",
       " (367907, 'local', 'arpa', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply `fmt_domain_graph_entry` to the raw data RDD\n",
    "formatted_host_counts = common_crawl_domain_counts\\\n",
    "        .map(lambda x: fmt_domain_graph_entry(x))\n",
    "\n",
    "# Display the first few entries of the new RDD\n",
    "formatted_host_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a9aae",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Your colleague has written another function called extract_domain_graph_host_count. \n",
    "\n",
    "Apply `extract_subdomain_counts` over `common_crawl_domain_counts` and save the result as a new RDD named `host_counts`.\n",
    "\n",
    "Study the function, what do you think the result will look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb75dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 7, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_subdomain_counts(entry):\n",
    "    \"\"\"\n",
    "    Extract the subdomain count from a Common Crawl domain graph entry.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')\n",
    "    \n",
    "    # return ONLY the num_subdomains\n",
    "    return int(num_subdomains)\n",
    "\n",
    "\n",
    "# Apply `extract_subdomain_counts` to the raw data RDD\n",
    "host_counts = common_crawl_domain_counts\\\n",
    "        .map(lambda x: extract_subdomain_counts(x))\n",
    "\n",
    "# Display the first few entries\n",
    "host_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44483f3",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Using `host_counts`, calculate the total number of subdomains across all domains in the dataset, save the result to a variable named `total_host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa284001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595466"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce the RDD to a single value, the sum of subdomains, with a lambda function\n",
    "# as the reduce function\n",
    "total_host_counts = host_counts\\\n",
    "        .reduce(lambda a, b: a + b)\n",
    "\n",
    "# Display result count\n",
    "total_host_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5579e",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "We can do a bit more analysis more easily with PySpark SQL. \n",
    "\n",
    "Stop the current `SparkSession` and `sparkContext` before moving on to analyze the data with SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the sparkContext and the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129687d",
   "metadata": {},
   "source": [
    "## Task Group 2 - Exploring Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22e2b6",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Create a new `SparkSession` and assign it to a variable named `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99565365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f3a0c",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Read `./crawl/cc-main-limited-domains.csv` into a new Spark DataFrame named `common_crawl`.\n",
    "\n",
    "This dataset doesn’t have headers, so we can use Spark’s auto-generated column names for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297084db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----+---+\n",
      "|_c0   |_c1        |_c2 |_c3|\n",
      "+------+-----------+----+---+\n",
      "|367855|172-in-addr|arpa|1  |\n",
      "|367856|addr       |arpa|1  |\n",
      "|367857|amphic     |arpa|1  |\n",
      "|367858|beta       |arpa|1  |\n",
      "|367859|callic     |arpa|1  |\n",
      "+------+-----------+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the target file into a DataFrame\n",
    "common_crawl = spark.read \\\n",
    "    .option('delimiter', '\\t') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv('./crawl/cc-main-limited-domains.csv')\n",
    "\n",
    "# Display the DataFrame to the notebook\n",
    "common_crawl.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23082fd2",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Because this dataset doesn’t have headers, we’ll have to set them ourselves. \n",
    "\n",
    "Rename the DataFrame's columns to the following: \n",
    "\n",
    "- site_id\n",
    "- domain\n",
    "- top_level_domain\n",
    "- num_subdomains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f7b4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------------+--------------+\n",
      "|site_id|domain     |top_level_domain|num_subdomains|\n",
      "+-------+-----------+----------------+--------------+\n",
      "|367855 |172-in-addr|arpa            |1             |\n",
      "|367856 |addr       |arpa            |1             |\n",
      "|367857 |amphic     |arpa            |1             |\n",
      "|367858 |beta       |arpa            |1             |\n",
      "|367859 |callic     |arpa            |1             |\n",
      "+-------+-----------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- top_level_domain: string (nullable = true)\n",
      " |-- num_subdomains: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the DataFrame's columns with `withColumnRenamed()`\n",
    "common_crawl = common_crawl\\\n",
    "    .withColumnRenamed('_c0', 'site_id')\\\n",
    "    .withColumnRenamed('_c1', 'domain')\\\n",
    "    .withColumnRenamed('_c2', 'top_level_domain')\\\n",
    "    .withColumnRenamed('_c3', 'num_subdomains')\n",
    "\n",
    "# Display the first few rows of the DataFrame and the new schema\n",
    "common_crawl.show(5, truncate=False)\n",
    "common_crawl.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff524e08",
   "metadata": {},
   "source": [
    "## Task Group 3 - Reading and Writing Datasets to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00bc518",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "\n",
    "Before moving on to analyzing this dataset, let’s save it as parquet files. This will help our other colleagues work with it more easily.\n",
    "\n",
    "Save the `common_crawl` DataFrame as parquet files in a directory called `./results/common_crawl/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33be3162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------------+--------------+\n",
      "|site_id|domain     |top_level_domain|num_subdomains|\n",
      "+-------+-----------+----------------+--------------+\n",
      "|367855 |172-in-addr|arpa            |1             |\n",
      "|367856 |addr       |arpa            |1             |\n",
      "|367857 |amphic     |arpa            |1             |\n",
      "|367858 |beta       |arpa            |1             |\n",
      "|367859 |callic     |arpa            |1             |\n",
      "+-------+-----------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the `common_crawl` DataFrame to a series of parquet files\n",
    "common_crawl.toDF('site_id', 'domain', 'top_level_domain', 'num_subdomains')\\\n",
    "    .show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbf8ff",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Read `./results/common_crawl/` into a new DataFrame to confirm our DataFrame was saved properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a99ceb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from parquet directory\n",
    "\n",
    "\n",
    "# Display the first few rows of the DataFrame and the schema in the notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f34ede",
   "metadata": {},
   "source": [
    "## Task Group 4 - Querying Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71895cf",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "Create a local temporary view from `common_crawl_domains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd04b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view in the metadata for this `SparkSession`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2ef4f",
   "metadata": {},
   "source": [
    "### Task 12\n",
    "\n",
    "Calculate the total number of domains for each top-level domain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f79679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the DataFrame using SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb11c33",
   "metadata": {},
   "source": [
    "### Task 13\n",
    "\n",
    "Calculate the total number of subdomains for each top-level domain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502578e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4539ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the DataFrame using SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2dfea3",
   "metadata": {},
   "source": [
    "### Task 14\n",
    "\n",
    "How many sub-domains does `nps.gov` have? Filter the dataset to that website's entry, display the columns `top_level_domain`, `domain`, and `num_subdomains` in your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45051e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame using DataFrame Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1746a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame using SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336d454",
   "metadata": {},
   "source": [
    "### Task 15\n",
    "\n",
    "Close the `SparkSession` and underlying `sparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2233037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the notebook's `SparkSession` and `sparkContext`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
